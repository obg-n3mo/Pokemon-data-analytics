{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Pokemon Tournament Data With BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to scrape data from regional level tournaments for the pokemon trading card game, and leave it in a format that will be useful for future projects. I eventually want to have a notebook set up where I can just type in the name of the tournament, press run, and I'll get out a csv that I can just upload to tableau and have a nice dashboard to provide some insight on the tournament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I can set the tournament url by specifying the city and the year. It would be nice to turn this into a web app at some point where the user is prompted to enter the year and the city, and everything else is automated. Or better yet, the rk9 page with the list of all tournament could be scraped to provide a dropdown menu of tournaments. However, that is beyond the scope of this project (for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the tournament name with the year and city of the regional\n",
    "\n",
    "tournament = 'pokemon-merida-2025'\n",
    "\n",
    "homepage = 'https://rk9.gg/event/' + tournament"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching URLs from the homepage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using BeautifulSoup we can locate the links for the roster and the pairings of the specified tounament. Note that the box with the links for tcg is sometimes labelled as \"indigo\" instead of \"blue\". This function will return the a list with the url for the roster as the first element and the url for the pairings in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rk9_urls(homepage):\n",
    "    soup = BeautifulSoup(requests.get(homepage).text)\n",
    "\n",
    "    tcg = soup.find(\"div\", class_ = 'card h-100 mt-3 p-2 shadow bg-blue-050') # Locate tcg box (sometimes indigo)\n",
    "\n",
    "    # Find url extensions for roster and pairings\n",
    "    roster_code = tcg.find('a', {'href': re.compile('/roster*')})['href']  \n",
    "    pairings_code = tcg.find('a', {'href': re.compile('/pairings*')})['href']\n",
    "\n",
    "    roster_url = 'https://rk9.gg' + roster_code\n",
    "    pairings_url = 'https://rk9.gg' + pairings_code\n",
    "    \n",
    "    return [roster_url,pairings_url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the roster of all players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The roster url leads to a table of all players in the tournament. As columns we have: player ID (only first and last digits are displayed), First Name, Last Name, Country, Division, Deck List (as a link), Standing.\n",
    "\n",
    "The following function will out put this table as either a csv or as a pandas dataframe for futher processing. In another project I'll write a classifier function that takes the deck list url and returns the archetype. This data can be combined with the pairings data to generate a matchup chart. Even better would be to filter the games included in the matchup data according to player elo, so all matchup data is high quality gameplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roster(homepage, csv = False, filename = tournament + 'roster.csv'):\n",
    "    \n",
    "    url = get_rk9_urls(homepage)[0] #set player roster url\n",
    "    \n",
    "    soup = BeautifulSoup(requests.get(url).text) #load in the soup\n",
    "\n",
    "    table = soup.find('table') #roster page only has one table\n",
    "    \n",
    "    headers = table.find_all('th') #Find the column headers\n",
    "    headers = [heading.string for heading in headers]\n",
    "    \n",
    "    body = table.find('tbody') # isolate the body of the table\n",
    "    \n",
    "    rows = body.find_all('tr') # get rows\n",
    "    \n",
    "    all_roster_data = []\n",
    "    for row in rows:  # This loop isolates the text in each cell\n",
    "\n",
    "        row_data = row.find_all('td')\n",
    "        individual_row_data = [data.text.strip() for data in row_data]\n",
    "\n",
    "        dlist = row_data[-2]\n",
    "        dlist_url = dlist.find('a')['href']  # This is grabbing the decklist url, otherwise you just get \"view\"\n",
    "\n",
    "        individual_row_data[-2] = dlist_url\n",
    "\n",
    "        all_roster_data.append(individual_row_data)\n",
    "        \n",
    "        df = pd.DataFrame(all_roster_data, columns = headers)\n",
    "        \n",
    "    if not csv:\n",
    "        return df #returns the desired table\n",
    "    if csv:\n",
    "        return df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the matchup data from each round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I'll create a function that queries the pairings page for a given round, and returns a dataframe of each game from that round along with the results.\n",
    "\n",
    "Some cleaning is also done in this function to deal with players dropping, getting DQd, or not showing up to the round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_round_pairings(round_number, total_rounds, pairings_url):\n",
    "    pairing_soup = BeautifulSoup(requests.get(pairings_url, {'pod' : '2', 'rnd' : str(round_number)}).text)\n",
    "    games = pairing_soup.find_all('div', class_ = \"row row-cols-3 match no-gutter complete\")\n",
    "    P1_names = [game.find('span', class_ = 'name').text for game in games]\n",
    "    P2_names = [game.find_all('span', class_ = 'name')[-1].text for game in games]\n",
    "\n",
    "    # Grab the match results, and deal with dropped players\n",
    "\n",
    "    P1_result = [game.find('div', class_ = re.compile(\"col-5 text-center player*\"))['class'][-1] for game in games]\n",
    "    for i in range(len(P1_result)):\n",
    "        game = games[i]\n",
    "        if P1_result[i] == 'dropped':\n",
    "            P1_result[i] = game.find('div', class_ = re.compile(\"col-5 text-center player*\"))['class'][4]\n",
    "\n",
    "        if P1_result[i] == 'dropped':\n",
    "            P1_result[i] = 'double game loss'\n",
    "\n",
    "    # Generate a DataFrame for the round\n",
    "\n",
    "    round_dict = {\n",
    "        'Player 1' : P1_names,\n",
    "        'Player 2' : P2_names,\n",
    "        'Result' : P1_result\n",
    "        }\n",
    "\n",
    "    round_df = pd.DataFrame(round_dict)\n",
    "    \n",
    "    mask = (round_df['Player 1'] == round_df['Player 2'])\n",
    "    \n",
    "    round_df = round_df[~mask]\n",
    "    \n",
    "    return round_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll use the above function to generate the dataframe for every round and concatenate them together. This is what we want for calculating matchup percentages, since we don't care during which round each game was played.\n",
    "\n",
    "However, this will need to be handled a bit differently for calculating Elos since the order of games played does matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairings_url = get_rk9_urls(homepage)[1]\n",
    "    \n",
    "total_soup = BeautifulSoup(requests.get(pairings_url).text)\n",
    "total_rounds = int(total_soup.find_all('a', id = re.compile('P2R*'))[-2].text[1:])\n",
    "all_matches_df = pd.DataFrame({}, columns = ['Player 1', 'Player 2', 'Result'])\n",
    "\n",
    "for round_number in range(1, total_rounds+1):\n",
    "    #print(round_number)\n",
    "    round_df = get_round_pairings(round_number, total_rounds, pairings_url)\n",
    "    all_matches_df = pd.concat([all_matches_df,round_df],axis = 0,ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can save everything as csv files for a tableau dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_roster(homepage, csv = True, filename = 'roster_for_'+tournament+'.csv')\n",
    "all_matches_df.to_csv('all_games_from_'+tournament+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
